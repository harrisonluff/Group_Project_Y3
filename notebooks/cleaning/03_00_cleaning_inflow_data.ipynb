{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29e7b67f",
   "metadata": {},
   "source": [
    "Code, takes list of file paths for the monthly inflow sheets -> returns final sheet with rows as powiat, date - cols for cumulative inflows. Can then pivot to what is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ee1d6a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo root: C:\\Users\\harri\\OneDrive - Imperial College London\\Year 3 Group Project\\Group_Project_Y3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "p = Path.cwd().resolve()\n",
    "repo_root = next((parent for parent in [p] + list(p.parents) if (parent / \".git\").exists()), None)\n",
    "if repo_root is None:\n",
    "    raise RuntimeError(\"Repo root not found. Open the repo folder in VS Code.\")\n",
    "\n",
    "sys.path.insert(0, str(repo_root))\n",
    "print(\"Repo root:\", repo_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a851a67",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eacf6a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afe72fb",
   "metadata": {},
   "source": [
    "Clean Individual df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2de80237",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_piece(x):\n",
    "    # Cleans multi-level column name piece\n",
    "    if pd.isna(x):\n",
    "        return \"\"\n",
    "    x = str(x).strip()\n",
    "    if x.startswith(\"Unnamed:\"):\n",
    "        return \"\"\n",
    "    x = re.sub(r\"\\s+\", \"_\", x)\n",
    "    return x\n",
    "\n",
    "def flatten_column_names(df):\n",
    "    # Convert multi-level column name to clean format\n",
    "    new_cols = []\n",
    "    for a, b, c in df.columns:\n",
    "        parts = [clean_piece(a), clean_piece(b), clean_piece(c)]\n",
    "        parts = [p for p in parts if p]  # drop blanks\n",
    "        col = \"_\".join(parts)\n",
    "        new_cols.append(col)\n",
    "    df = df.copy()\n",
    "    df.columns = new_cols\n",
    "    return df\n",
    "\n",
    "def translate_col(x):\n",
    "    # Translate single column name according to mapping\n",
    "    map = {\n",
    "        \"TERYT\": \"teryt\",\n",
    "        \"POWIAT\": \"powiat\",\n",
    "        \"WOJEWÓDZTWO\": \"voivodeship\",\n",
    "        \"WOJEWODZTWO\": \"voivodeship\",\n",
    "        \"ŁĄCZNIE\": \"total\",\n",
    "        \"LACZNIE\": \"total\",\n",
    "        \"KOBIET\": \"female\",       # appears as \"KOBIET\" in your header\n",
    "        \"KOBIETY\": \"female\",\n",
    "        \"MĘŻCZYZN\": \"male\",       # appears as \"MĘŻCZYZN\" in your header\n",
    "        \"MEZCZYZN\": \"male\",\n",
    "        \"MĘŻCZYŹNI\": \"male\",\n",
    "        \"MEZCZYZNI\": \"male\",\n",
    "        \"WSZYSTKICH\": \"all\",\n",
    "        # \"ROK_URODZENIA\": \"birth_year\",\n",
    "        # \"ROK\": \"year\",\n",
    "        # \"URODZENIA\": \"birth\",\n",
    "        \"ROK_URODZENIA\": \"\", # just to replace this with none\n",
    "        \"ROK\": \"\",\n",
    "        \"URODZENIA\": \"\",\n",
    "    } # thank you chat\n",
    "\n",
    "    parts = x.split(\"_\")\n",
    "    parts = [map.get(p, p.lower()) for p in parts]\n",
    "\n",
    "    out = \"_\".join([p for p in parts if p])\n",
    "    return out\n",
    "\n",
    "def translate_columns(df):\n",
    "    # Translate column names to english\n",
    "    out = df.copy()\n",
    "    out.columns = [translate_col(c).replace(\"...\", \"1900\") for c in out.columns] # the replace just turns ...-1958 to 1900-1958\n",
    "    return out\n",
    "\n",
    "# ========================================================\n",
    "\n",
    "def fill_lt2_equal_share(df, total_col, prefix, lt2_token=\"<2\"):\n",
    "    # Takes total column and prefix for individual columns which should sum to it\n",
    "    # Assigns <2 with numeric value equal to share of missing total for that row\n",
    "    out = df.copy()\n",
    "\n",
    "    # get individual cols\n",
    "    cols = [c for c in out.columns if c.startswith(prefix)]\n",
    "    \n",
    "    # total as numeric\n",
    "    out[total_col] = pd.to_numeric(out[total_col], errors=\"coerce\")\n",
    "\n",
    "    # mask for cells that are \"<2\"\n",
    "    lt2_mask = out[cols].astype(str).eq(lt2_token)\n",
    "\n",
    "    # convert cols to numeric, \"<2\" becomes NaN\n",
    "    # cols_num = out[cols].replace(lt2_token, np.nan)\n",
    "    cols_num = out[cols].mask(lt2_mask)\n",
    "    cols_num = cols_num.apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "    # row sums\n",
    "    known_sum = cols_num.sum(axis=1, skipna=True)\n",
    "\n",
    "    # calculate missing share for row\n",
    "    missing_total = out[total_col] - known_sum\n",
    "    k = lt2_mask.sum(axis=1)\n",
    "    share = missing_total.div(k).where(k>0)\n",
    "\n",
    "    # fill \"<2\" with computed share\n",
    "    filled = cols_num.copy()\n",
    "    for col in cols:\n",
    "        filled[col] = np.where(lt2_mask[col], share, filled[col]) # if lt2 mask then share, else take existing value\n",
    "    \n",
    "    out[cols] = filled\n",
    "    return out\n",
    "\n",
    "# ========================================================\n",
    "\n",
    "def birth_to_age_label(col, ref_year):\n",
    "    # Convert columns like:\n",
    "    #     male_2026 -> male_0\n",
    "    #     male_1900-1958 -> male_68-126   \n",
    "    # using age = ref_year - birth_year (assuming birth at year start)\n",
    "    \n",
    "    m = re.match(r\"^(male|female)_(\\d{4})(?:-(\\d{4}))?$\", col)\n",
    "    if not m:\n",
    "        return col\n",
    "\n",
    "    sex, y1, y2 = m.group(1), int(m.group(2)), m.group(3)\n",
    "\n",
    "    # ie only a single year column\n",
    "    if y2 is None:\n",
    "        age = ref_year - y1\n",
    "        return f\"{sex}_{age}\"\n",
    "\n",
    "    # otherwise it is a range column\n",
    "    y2 = int(y2)\n",
    "    age_high = ref_year - y1 # y1 lower year -> higher age\n",
    "    age_low  = ref_year - y2\n",
    "    return f\"{sex}_{age_low}-{age_high}\"\n",
    "\n",
    "def convert_birthyear_cols_to_age(df, date_col=\"date\"):\n",
    "    # Rename male_* and female_* birth-year columns to age columns using date column year\n",
    "    out = df.copy()\n",
    "    ref_year = int(pd.to_datetime(out[date_col]).dt.year.dropna().iloc[0])\n",
    "\n",
    "    rename_map = {}\n",
    "    for c in out.columns:\n",
    "        rename_map[c] = birth_to_age_label(c, ref_year)\n",
    "\n",
    "    out = out.rename(columns=rename_map)\n",
    "    return out\n",
    "\n",
    "# ========================================================\n",
    "\n",
    "def clean_single_sheet(path, incl_2026=True):\n",
    "    datestring = path.split(\"_\")[-1].replace(\".xlsx\", \"\")\n",
    "    print(f\"Processing {datestring}\")\n",
    "\n",
    "    if datestring[0:4]==\"2026\" and not incl_2026:\n",
    "        return None\n",
    "\n",
    "    # Clean df formatting\n",
    "    df = pd.read_excel(path, header=[0, 1, 2])\n",
    "    df = flatten_column_names(df) # flatten multi level column name\n",
    "    df = translate_columns(df) # translate cols to english\n",
    "    df[\"date\"] = pd.to_datetime(datestring, format=\"%Y%m%d\") # add column for date\n",
    "\n",
    "    # Process the <2 and convert to ints\n",
    "    df = fill_lt2_equal_share(df, \"total_female\", \"female\")\n",
    "    df = fill_lt2_equal_share(df, \"total_male\", \"male\")\n",
    "\n",
    "    # Convert cols to age, i.e. if 2026 male_2026 becomes male_0, this allows stacking of dfs (hopefully) - it doesn't exactly but useful still\n",
    "    df = convert_birthyear_cols_to_age(df, date_col=\"date\")\n",
    "\n",
    "    # Return df\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f454849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_single_sheet(f\"{str(repo_root)}/raw/01_inflow_data/STATYSTYKI_POWIAT_UKR_20220414.xlsx\") # Uncomment to see example process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9172d5ab",
   "metadata": {},
   "source": [
    "Get all xlsx in folder and process into one stacked df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e35d82b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 58 files.\n",
      "Processing 20220414\n",
      "Processing 20220426\n",
      "Processing 20220517\n",
      "Processing 20220601\n",
      "Processing 20220615\n",
      "Processing 20220701\n",
      "Processing 20220714\n",
      "Processing 20220801\n",
      "Processing 20220901\n",
      "Processing 20220915\n",
      "Processing 20220928\n",
      "Processing 20221031\n",
      "Processing 20221130\n",
      "Processing 20221227\n",
      "Processing 20230131\n",
      "Processing 20230228\n",
      "Processing 20230328\n",
      "Processing 20230425\n",
      "Processing 20230530\n",
      "Processing 20230627\n",
      "Processing 20230704\n",
      "Processing 20230711\n",
      "Processing 20230718\n",
      "Processing 20230725\n",
      "Processing 20230801\n",
      "Processing 20230808\n",
      "Processing 20230816\n",
      "Processing 20230822\n",
      "Processing 20230927\n",
      "Processing 20231010\n",
      "Processing 20231114\n",
      "Processing 20231212\n",
      "Processing 20240109\n",
      "Processing 20240213\n",
      "Processing 20240312\n",
      "Processing 20240409\n",
      "Processing 20240514\n",
      "Processing 20240611\n",
      "Processing 20240709\n",
      "Processing 20240813\n",
      "Processing 20240910\n",
      "Processing 20241008\n",
      "Processing 20241112\n",
      "Processing 20241210\n",
      "Processing 20250114\n",
      "Processing 20250211\n",
      "Processing 20250311\n",
      "Processing 20250407\n",
      "Processing 20250513\n",
      "Processing 20250610\n",
      "Processing 20250708\n",
      "Processing 20250812\n",
      "Processing 20250909\n",
      "Processing 20251014\n",
      "Processing 20251021\n",
      "Processing 20251112\n",
      "Processing 20251209\n",
      "Processing 20260113\n",
      "\n",
      "Processed 58 files.\n"
     ]
    }
   ],
   "source": [
    "folder = repo_root / \"raw/03_00_inflow_data\"\n",
    "\n",
    "xlsx_files = [str(p) for p in folder.glob(\"*.xlsx\")]\n",
    "\n",
    "print(f\"Found {len(xlsx_files)} files.\")\n",
    "\n",
    "dfs = []\n",
    "for path in xlsx_files:\n",
    "    df = clean_single_sheet(path, incl_2026=True)\n",
    "    if df is not None:\n",
    "        dfs.append(df)\n",
    "    else:\n",
    "        print(\"Skipped - 2026 file!\")\n",
    "\n",
    "print(f\"\\nProcessed {len(dfs)} files.\")\n",
    "\n",
    "df_all = pd.concat(dfs, ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8ef7c3",
   "metadata": {},
   "source": [
    "Now bin by ages (assuming uniform distribution on overlaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7471a3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_age_col(col):\n",
    "    # Parse 'male_7' -> ('male', 7, 7)\n",
    "    #       'female_20-24' -> ('female', 20, 24)\n",
    "    #       'male_65+' -> ('male', 65, 200)\n",
    "    # Returns None if not an age column.\n",
    "    \n",
    "    m = re.match(r'^(male|female)_(\\d+)(?:-(\\d+))?$', col)\n",
    "    if m:\n",
    "        sex = m.group(1)\n",
    "        lo = int(m.group(2))\n",
    "        hi = int(m.group(3)) if m.group(3) else lo\n",
    "        return sex, lo, hi\n",
    "\n",
    "    m = re.match(r'^(male|female)_(\\d+)\\+$', col)\n",
    "    if m:\n",
    "        sex = m.group(1)\n",
    "        lo = int(m.group(2))\n",
    "        return sex, lo, 200\n",
    "\n",
    "    return None\n",
    "\n",
    "def overlap_len(a_lo, a_hi, b_lo, b_hi):\n",
    "    lo = max(a_lo, b_lo)\n",
    "    hi = min(a_hi, b_hi)\n",
    "    return max(0, hi - lo + 1)\n",
    "\n",
    "def target_colname(sex: str, lo: int, hi: int):\n",
    "    if lo == hi:\n",
    "        return f\"{sex}_{lo}\"\n",
    "    if hi >= 200:\n",
    "        return f\"{sex}_{lo}+\"\n",
    "    return f\"{sex}_{lo}-{hi}\"\n",
    "\n",
    "# ======================================================\n",
    "\n",
    "def harmonise_age_bins_by_overlap(\n",
    "    df_all,\n",
    "    target_bins,\n",
    "    id_cols=(\"date\", \"teryt\", \"powiat\", \"voivodeship\", \"total_all\", \"total_male\", \"total_female\"),\n",
    "    sexes=(\"male\", \"female\"),\n",
    "    cap_hi=200,\n",
    "):\n",
    "    # For each sex and each target bin, sum contributions from all overlapping source age columns\n",
    "    # in df_all, proportional to overlap width / source width.\n",
    "    \n",
    "    out = df_all.copy()\n",
    "\n",
    "    # collect all source age columns for each sex\n",
    "    src_by_sex = {sex: [] for sex in sexes}\n",
    "    for c in out.columns:\n",
    "        parsed = parse_age_col(c)\n",
    "        if parsed and parsed[0] in sexes:\n",
    "            sex, lo, hi = parsed\n",
    "            hi = min(hi, cap_hi)\n",
    "            src_by_sex[sex].append((c, lo, hi))\n",
    "\n",
    "    # prepare result df with just ids\n",
    "    keep_ids = [c for c in id_cols if c in out.columns]\n",
    "    res = out.loc[:, keep_ids].copy()\n",
    "\n",
    "    # build target columns by overlap-summing\n",
    "    for sex in sexes:\n",
    "        # numeric view of source cols (missing -> 0)\n",
    "        src_cols = [c for c, _, _ in src_by_sex[sex]]\n",
    "        if not src_cols:\n",
    "            # still create empty target cols - probably shouldnt be any though\n",
    "            for lo, hi in target_bins:\n",
    "                res[target_colname(sex, lo, hi)] = np.nan\n",
    "            continue\n",
    "\n",
    "        src_vals = out[src_cols].apply(pd.to_numeric, errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "        for t_lo, t_hi in target_bins:\n",
    "            t_hi = min(t_hi, cap_hi)\n",
    "            tcol = target_colname(sex, t_lo, t_hi)\n",
    "\n",
    "            acc = np.zeros(len(out), dtype=float)\n",
    "\n",
    "            for c, s_lo, s_hi in src_by_sex[sex]:\n",
    "                ol = overlap_len(s_lo, s_hi, t_lo, t_hi)\n",
    "                if ol == 0:\n",
    "                    continue\n",
    "                width = (s_hi - s_lo + 1)\n",
    "                frac = ol / width\n",
    "                acc += src_vals[c].to_numpy() * frac\n",
    "\n",
    "            res[tcol] = acc\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f11f2578",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_BINS = (\n",
    "    [(i, i) for i in range(0, 20)] +\n",
    "    [(20, 24), (25, 29), (30, 34), (35, 39),\n",
    "     (40, 44), (45, 49), (50, 54), (55, 59),\n",
    "     (60, 64), (65, 200)] # set last to 200 for the 65+ col\n",
    ")\n",
    "\n",
    "df_harmonised = harmonise_age_bins_by_overlap(df_all, TARGET_BINS)\n",
    "\n",
    "# set original columns to float for consistency\n",
    "num_cols = df_harmonised.select_dtypes(include=[\"number\"]).columns\n",
    "df_harmonised[num_cols] = df_harmonised[num_cols].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b763aad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True    22040\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check sum over bins correct\n",
    "male_targets = [c for c in df_harmonised.columns if c.startswith(\"male_\")]\n",
    "female_targets = [c for c in df_harmonised.columns if c.startswith(\"female_\")]\n",
    "\n",
    "df_harmonised[\"sum_male\"] = df_harmonised[male_targets].sum(axis=1) \n",
    "df_harmonised[\"sum_female\"] = df_harmonised[female_targets].sum(axis=1)\n",
    "\n",
    "mask = (df_harmonised[\"total_female\"].round(2) == df_harmonised[\"sum_female\"].round(2))\n",
    "mask.value_counts()\n",
    "# df_harmonised[mask==False][[\"total_female\", \"sum_female\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "842fa751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the sum columns and then export to csv\n",
    "df_harmonised.drop([\"sum_male\", \"sum_female\"], axis=1, inplace=True)\n",
    "\n",
    "df_harmonised.to_csv(f\"{str(repo_root)}/cleaned/01_inflow_data/inflow_semi_cleaned.csv\", index=False, encoding=\"utf-8-sig\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
